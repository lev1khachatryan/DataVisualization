{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">Latent Dirichlet Allocation(LDA): Topic Modeling</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Levon Khachatryan:\n",
    "  \n",
    "  \n",
    " <img src=\"pics/LDA.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"top\"></a> <br>\n",
    "## Notebook  Content\n",
    "1. [LDA Algorithm](#1)\n",
    "  \n",
    "  \n",
    "2. [Problem Definition](#2)\n",
    "  \n",
    "  \n",
    "3. [Import Packages](#3)\n",
    "  \n",
    "  \n",
    "4. [Load Data](#4)\n",
    "  \n",
    "  \n",
    "5. [Used functions for Data Preprocessing](#5)\n",
    "  \n",
    "  \n",
    "6. [Data Preprocessing](#6)\n",
    "  \n",
    "  \n",
    "7. [Model Deployment](#7)\n",
    "  \n",
    "  \n",
    "8. [Save Model to Disk](#8)\n",
    "  \n",
    "  \n",
    "9. [Load Model From Disk](#9)\n",
    "  \n",
    "  \n",
    "10. [Detailed Information of Topics](#10)\n",
    "  \n",
    "  \n",
    "11. [Word Cloud](#11)\n",
    "  \n",
    "  \n",
    "12. [Message Analysis by Country](#12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">1. LDA Algorithm</div>\n",
    "---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "Topic modeling is the process of identifying topics in a set of documents. This can be useful for search engines, customer service automation, and any other instance where knowing the topics of documents is important. There are multiple methods of going about doing this, but here I will explain one: Latent Dirichlet Allocation (LDA).  \n",
    "  \n",
    "  \n",
    "  \n",
    "### The Algorithm\n",
    "LDA is a form of unsupervised learning that views documents as bags of words (**ie order does not matter**). LDA works by first making a key assumption: the way a document was generated was by picking a set of topics and then for each topic picking a set of words. Now you may be asking “ok so how does it find topics?” Well the answer is simple: it reverse engineers this process. To do this it does the following for each document m:  \n",
    "  \n",
    "1. Assume there are k topics across all of the documents\n",
    "2. Distribute these k topics across document m (this distribution is known as **α** and can be symmetric or asymmetric, more on this later) by assigning each word a topic.\n",
    "3. For each word w in document m, assume its topic is wrong but every other word is assigned the correct topic.\n",
    "4. Probabilistically assign word w a topic based on two things:\n",
    "    1. what topics are in document m\n",
    "    2. how many times word w has been assigned a particular topic across all of the documents (this distribution is called β, more on this later)\n",
    "5. Repeat this process a number of times for each document and you’re done!\n",
    "  \n",
    "  \n",
    "  \n",
    "### The Model\n",
    "<img src=\"pics/model.png\" />  \n",
    "  \n",
    "Smoothed LDA from https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  \n",
    "  \n",
    "Above is what is known as a plate diagram of an LDA model where:  \n",
    "α is the per-document topic distributions,  \n",
    "β is the per-topic word distribution,  \n",
    "θ is the topic distribution for document m,  \n",
    "φ is the word distribution for topic k,  \n",
    "z is the topic for the n-th word in document m, and  \n",
    "w is the specific word  \n",
    "  \n",
    "  \n",
    "  \n",
    "### Tweaking the Model\n",
    "In the plate model diagram above, you can see that w is grayed out. This is because it is the only observable variable in the system while the others are latent. Because of this, to tweak the model there are a few things you can mess with and below I focus on two.  \n",
    "  \n",
    "  \n",
    "α is a matrix where each row is a document and each column represents a topic. A value in row i and column j represents how likely document i contains topic j. A symmetric distribution would mean that each topic is evenly distributed throughout the document while an asymmetric distribution favors certain topics over others. This affects the starting point of the model and can be used when you have a rough idea of how the topics are distributed to improve results.  \n",
    "  \n",
    "  \n",
    "β is a matrix where each row represents a topic and each column represents a word. A value in row i and column j represents how likely that topic i contains word j. Usually each word is distributed evenly throughout the topic such that no topic is biased towards certain words. This can be exploited though in order to bias certain topics to favor certain words. For example if you know you have a topic about Apple products it can be helpful to bias words like “iphone” and “ipad” for one of the topics in order to push the model towards finding that particular topic.  \n",
    "  \n",
    "  \n",
    "  \n",
    "### Conclusion\n",
    "This part is not meant to be a full-blown LDA tutorial, but rather to give an overview of how LDA models work and how to use them. There are many implementations out there such as Gensim that are easy to use and very effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">2. Problem Definition</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have a text data (from messenger) and our aim is to find some topics from data (do topic modeling). Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">3. Import Packages</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loading numpy and pandas libraries\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import * \n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "'''\n",
    "Load english wards from nltk , and english stemmers\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "'''\n",
    "Load Regular expressions\n",
    "'''\n",
    "import re\n",
    "\n",
    "\n",
    "'''\n",
    "Load operator package, this will be used in dictionary sort\n",
    "'''\n",
    "import operator\n",
    "\n",
    "\n",
    "'''\n",
    "fix random state\n",
    "'''\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "'''\n",
    "Suppress warnings\n",
    "'''\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "'''\n",
    "Load punctuation for data preprocesing\n",
    "'''\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "'''\n",
    "Word cloud implementation\n",
    "'''\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">4. Load Data</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After delete unused messages the Remaining count of conversation is: 24860\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load data from csv file, which is in the same folder\n",
    "'''\n",
    "data = pd.read_csv('***.csv')\n",
    "\n",
    "\n",
    "'''\n",
    "Delete messages created by ***\n",
    "'''\n",
    "data = data[data.u_id != 1]\n",
    "\n",
    "\n",
    "'''\n",
    "Correct the Date column format\n",
    "'''\n",
    "data.date = data.date.str.slice(0, 10)\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "'''\n",
    "Choose only same part from data, in this example I Chose the messages created on last month\n",
    "'''\n",
    "data = data.loc[data.date >= '20190407']\n",
    "\n",
    "\n",
    "'''\n",
    "Delete messages containing no more than 3 characters\n",
    "'''\n",
    "data = data[data.text.str.len() > 3]\n",
    "\n",
    "\n",
    "'''\n",
    "Remaining conversations\n",
    "'''\n",
    "print('After delete unused messages the Remaining count of conversation is: {}'.format(data.c_id.nunique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group messages to appropriate conversations which we will consider as documents\n",
    "'''\n",
    "conversation = data.groupby('c_id')['text'].apply(lambda x: \"%s\" % ', '.join(x))\n",
    "documents = conversation.to_frame(name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">5. Used Functions for Data Preprocessing</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    \"\"\" \n",
    "    Word preprocessing \n",
    "  \n",
    "    This function will preprocess particular word \n",
    "  \n",
    "    Parameters: \n",
    "    word: string\n",
    "  \n",
    "    Returns: \n",
    "    string: will return initial string input but preprocessed ,\n",
    "            so from input string will delete all punctuation and repeated symbols.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Remove punctuation\n",
    "    word = ''.join(c for c in word if c not in punctuation)\n",
    "    \n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    \n",
    "    return word\n",
    "\n",
    "# preprocess_word('aaa|sd''f,gh!jg&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_word(word):\n",
    "    \"\"\" \n",
    "    Word checking\n",
    "  \n",
    "    This function will check if word starts with alphabet \n",
    "  \n",
    "    Parameters: \n",
    "    word: string\n",
    "  \n",
    "    Returns: \n",
    "    Boolean: Is valid or not , True means that word is valid\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "# is_valid_word('1dgh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(document):\n",
    "    \"\"\" \n",
    "    Emoji classifier\n",
    "  \n",
    "    This function will replace emojis with EMO_POS or EMO_NEG , depending on its meaning \n",
    "  \n",
    "    Parameters: \n",
    "    document: string\n",
    "  \n",
    "    Returns: \n",
    "    string: initial string input replaced emojis by their meaning, \n",
    "            for example :) will replaced with EMO_POS but ): will replaced with EMO_NEG\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    document = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', document)\n",
    "    \n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    document = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', document)\n",
    "    \n",
    "    # Love -- <3, :*\n",
    "    document = re.sub(r'(<3|:\\*)', ' EMO_POS ', document)\n",
    "    \n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    document = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', document)\n",
    "    \n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    document = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', document)\n",
    "    \n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    document = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', document)\n",
    "    \n",
    "    return document\n",
    "\n",
    "# handle_emojis('dsf):ghj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document, use_stemmer = False):\n",
    "    \"\"\" \n",
    "    Text preprocessing\n",
    "  \n",
    "    This function will preprocess the input text \n",
    "  \n",
    "    Parameters: \n",
    "    document: string (we can put the entire string row , for instance in our case I will pass conversation)\n",
    "    use_stemmer: Boolean (If True I will use stemmer as well as all other processes)\n",
    "  \n",
    "    Returns: \n",
    "    string: processed input string\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "    processed_document = []\n",
    "    \n",
    "    # Convert to lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Replaces URLs with the word URL\n",
    "    document = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', document)\n",
    "    \n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    document = re.sub(r'@[\\S]+', 'USER_MENTION', document)\n",
    "    \n",
    "    # Replaces #hashtag with hashtag\n",
    "    document = re.sub(r'#(\\S+)', r' \\1 ', document)\n",
    "    \n",
    "    # Replace 2+ dots with space\n",
    "    document = re.sub(r'\\.{2,}', ' ', document)\n",
    "    \n",
    "    # Strip space, \" and ' from document\n",
    "    document = document.strip(' \"\\'')\n",
    "    \n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    document = handle_emojis(document)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    words = document.split()\n",
    "\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            if use_stemmer:\n",
    "                word = lemmatize_stemming(word)\n",
    "            if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 3:\n",
    "                processed_document.append(word)\n",
    "            \n",
    "    processed_internal_state = ' '.join(processed_document)\n",
    "    \n",
    "    processed_internal_state = re.sub(r'\\b\\w{1,3}\\b', '', processed_internal_state)\n",
    "    \n",
    "    processed_internal_state = ' '.join(processed_internal_state.split())\n",
    "\n",
    "    return processed_internal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(preprocessed_document):\n",
    "    \"\"\" \n",
    "    tokenize and combine already preprocessed document\n",
    "  \n",
    "    This function will tokenize document and will combine document such a way ,\n",
    "    that we can containing the number of times a word appears in the training set \n",
    "    using gensim.corpora.Dictionary\n",
    "  \n",
    "    Parameters: \n",
    "    preprocessed_document: string (particular document obtained from preprocess_document function)\n",
    "  \n",
    "    Returns: \n",
    "    list: tokenized documents in approprite form\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    result=[]\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(preprocessed_document) :\n",
    "        result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">6. Data Preprocessing</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a list from 'documents' DataFrame and call it 'processed_docs'\n",
    "'''\n",
    "\n",
    "processed_docs = []\n",
    "\n",
    "for doc in documents.values:\n",
    "    processed_docs.append(preprocess(preprocess_document(doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the data set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Checking dictionary created\n",
    "# '''\n",
    "\n",
    "# count = 0\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     print(k, v)\n",
    "#     count += 1\n",
    "#     if count > 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Preview BOW for our sample preprocessed document\n",
    "# '''\n",
    "\n",
    "# document_num = 50\n",
    "# bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "# for i in range(len(bow_doc_x)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "#                                                      dictionary[bow_doc_x[i][0]], \n",
    "#                                                      bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">7. Model Deployment</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Latent Dirichlet Allocation (LDA) in Python, using all CPU cores to parallelize and speed up model training.  \n",
    "  \n",
    "The parallelization uses multiprocessing; in case this doesn’t work for you for some reason, try the gensim.models.ldamodel.LdaModel class which is an equivalent, but more straightforward and single-core implementation.  \n",
    "  \n",
    "  \n",
    "The training algorithm:\n",
    "1. is streamed: training documents may come in sequentially, no random access required,\n",
    "2. runs in constant memory w.r.t. the number of documents: size of the training corpus does not affect memory footprint, can process corpora larger than RAM  \n",
    "  \n",
    "  \n",
    "This module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents. The model can also be updated with new documents for online training.  \n",
    "  \n",
    "  \n",
    "class **gensim.models.ldamulticore.LdaMulticore**(corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=<type 'numpy.float32'>)  \n",
    "  \n",
    "  \n",
    "Bases: gensim.models.ldamodel.LdaModel  \n",
    "  \n",
    "An optimized implementation of the LDA algorithm, able to harness the power of multicore CPUs. Follows the similar API as the parent class LdaModel.    \n",
    "  \n",
    "  \n",
    "**Parameters:**\n",
    "  \n",
    "1. corpus ({iterable of list of (int, float), scipy.sparse.csc}, optional) – Stream of document vectors or sparse matrix of shape (num_terms, num_documents). If not given, the model is left untrained (presumably because you want to call update() manually).\n",
    "2. num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
    "3. id2word ({dict of (int, str), gensim.corpora.dictionary.Dictionary}) – Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "4. workers (int, optional) – Number of workers processes to be used for parallelization. If None all available cores (as estimated by workers=cpu_count()-1 will be used. Note however that for hyper-threaded CPUs, this estimation returns a too high number – set workers directly to the number of your real cores (not hyperthreads) minus one, for optimal performance.\n",
    "5. chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "6. passes (int, optional) – Number of passes through the corpus during training.\n",
    "7. alpha ({np.ndarray, str}, optional) – Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string: ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n",
    "8. gamma_threshold (float, optional) – Minimum change in the value of the gamma parameters to continue iterating.\n",
    "9. minimum_probability (float, optional) – Topics with a probability lower than this threshold will be filtered out.\n",
    "10. per_word_topics (bool) – If True, the model also computes a list of topics, sorted in descending order of most likely topics for each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
    "11. minimum_phi_value (float, optional) – if per_word_topics is True, this represents a lower bound on the term probabilities.\n",
    "  \n",
    "  \n",
    "**Methods and functions**\n",
    "  \n",
    "  \n",
    "There are varous methods for lda model which we can find in the lda documentation:   https://radimrehurek.com/gensim/models/ldamulticore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "number_of_topics = 7\n",
    "\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = number_of_topics, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">8. Save Model to Disk</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save model to disk.\n",
    "'''\n",
    "\n",
    "directory_to_save = 'C:\\\\_Files\\\\MyProjects\\\\***_TopicExtraction\\\\model\\\\model'\n",
    "lda_model.save(directory_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">9. Load Model From Disk</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load a potentially pretrained model from disk.\n",
    "'''\n",
    "lda_model = gensim.models.LdaMulticore.load(directory_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">10. Detailed Information of Topics</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24860, 7)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create (num_of_conv x num_of_topic) matrix with all 0 values and call it conversation_topic\n",
    "'''\n",
    "\n",
    "conversation_topic = np.zeros(shape=(len(bow_corpus), number_of_topics), dtype=float)\n",
    "print(conversation_topic.shape)\n",
    "print(conversation_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fill appropriate probability of conversation i to belong topic j to conversation_topic matrix\n",
    "'''\n",
    "\n",
    "for i in range(len(bow_corpus)):\n",
    "    prob = lda_model.get_document_topics(bow_corpus[i], per_word_topics = False)\n",
    "    for k in range(len(prob)):\n",
    "        conversation_topic[i, prob[k][0]] = prob[k][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate summed probabilities of each topic and call it prob_dict\n",
    "'''\n",
    "\n",
    "prob_dict = dict()\n",
    "for i in range(number_of_topics):\n",
    "    prob_dict[i] = round(conversation_topic.sum(axis = 0)[i] / len(bow_corpus), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0.08), (1, 0.09), (2, 0.09), (0, 0.14), (3, 0.15), (4, 0.21), (5, 0.23)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Sort prob_dict dictionary t find the most probable topic over all conversation dataset\n",
    "'''\n",
    "\n",
    "sorted_prob = sorted(prob_dict.items(), key=operator.itemgetter(1))\n",
    "print(sorted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">11. Word Cloud</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Combine all preprocessed conversations to one string and call it word_cloud_messenger\n",
    "'''\n",
    "\n",
    "word_cloud_messenger = []\n",
    "\n",
    "for doc in processed_docs:\n",
    "    s = \" \"\n",
    "    word_cloud_messenger.append(s.join( doc ))\n",
    "\n",
    "s = \" \"\n",
    "\n",
    "word_cloud_messenger = s.join( word_cloud_messenger )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save generated word cloud to disk\n",
    "'''\n",
    "\n",
    "np.save('word_cloud_messenger.npy', word_cloud_messenger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read generated word cloud from disk\n",
    "'''\n",
    "\n",
    "word_cloud_messenger = np.load('word_cloud_messenger.npy')\n",
    "word_cloud_messenger = str(word_cloud_messenger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate Picture of words, so called word cloud\n",
    "'''\n",
    "\n",
    "# Create stopword list:\n",
    "stopwords = set()\n",
    "stopwords.update([\"doritos\", \"doritosdoritos\", \"chirp\", \"chirpchirp\", \"mexico\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, \n",
    "                      background_color=\"white\",\n",
    "                      width = 800, \n",
    "                      height = 800, \n",
    "                      min_font_size = 10).generate(word_cloud_messenger)\n",
    "\n",
    "# Save the image in the img folder:\n",
    "wordcloud.to_file(\"first_review.png\")\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate Picture of words following a color pattern (with mask).\n",
    "'''\n",
    "\n",
    "# Create stopword list:\n",
    "stopwords = set()\n",
    "stopwords.update([\"doritos\", \"doritosdoritos\", \"chirp\", \"chirpchirp\", \"mexico\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "mask = np.array(Image.open(\"Icon.jpg\"))\n",
    "wordcloud_ddxk_learn = WordCloud(stopwords=stopwords, \n",
    "                                 background_color=\"white\", \n",
    "                                 mode=\"RGBA\", \n",
    "                                 max_words=1000,\n",
    "#                                  width = 800, \n",
    "#                                  height = 800, \n",
    "#                                  min_font_size = 10,\n",
    "                                 mask=mask).generate(word_cloud_messenger)\n",
    "\n",
    "# create coloring from image\n",
    "image_colors = ImageColorGenerator(mask)\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.imshow(wordcloud_ddxk_learn.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# store to file\n",
    "plt.savefig(\"second_review.png\", format=\"png\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "\n",
    "# <div align=\"center\">12. Message Analysis by Country</div>\n",
    "---------------------------------------------------------------------\n",
    "[go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
